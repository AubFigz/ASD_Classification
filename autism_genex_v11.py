# -*- coding: utf-8 -*-
"""Autism_GeneX_v11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rDtZyIZQanzIDaT8sEFubDBoS03_YkRF
"""

!pip install shap xgboost catboost optuna

from google.colab import drive
# Mount Google Drive
drive.mount('/content/drive')

import time
start_time = time.time()

# Import necessary libraries
import pandas as pd  # For data manipulation and analysis
import numpy as np  # For numerical operations
import seaborn as sns  # For data visualization
import matplotlib.pyplot as plt  # For plotting graphs
from sklearn.impute import KNNImputer  # For imputing missing values
from scipy.stats import ttest_ind  # For performing t-tests
from statsmodels.stats.multitest import multipletests  # For multiple testing correction
from sklearn.preprocessing import StandardScaler  # For feature scaling
from imblearn.over_sampling import SMOTE  # For handling imbalanced data
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split  # For model validation and data splitting
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report  # For model evaluation metrics
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # For ensemble models
from xgboost import XGBClassifier  # For XGBoost model
from catboost import CatBoostClassifier  # For CatBoost model
from sklearn.svm import SVC  # For Support Vector Machine
from sklearn.neural_network import MLPClassifier  # For Multi-layer Perceptron
from sklearn.feature_selection import RFE  # For Recursive Feature Elimination
from sklearn.decomposition import PCA  # For Principal Component Analysis
import optuna  # For hyperparameter optimization
import itertools  # For generating combinations
import logging  # For logging information
import shap  # For SHapley Additive exPlanations
from datetime import datetime # For logging dates and times
import os # For interacting with the operating system

# Ensure the directory exists
log_dir = '/content/logs'
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

# Clear any existing logging configurations
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)

# Set up logging
log_file_path = os.path.join(log_dir, 'run_log_01.log')
logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s - %(message)s')

# Verify log file creation
if os.path.exists(log_file_path):
    print(f"Log file created successfully at {log_file_path}")
else:
    print("Failed to create log file.")

# Define the paths to the CSV files
disease_state_path = '/content/drive/My Drive/Autism/Disease_State.csv'
gds_path = '/content/drive/My Drive/Autism/GDS4431.csv'

# Load and explore data
disease_state_df = pd.read_csv(disease_state_path).set_index('samples')
gds_df = pd.read_csv(gds_path).set_index('ID_REF')

# Print total number of samples and genes before removing any NaN values
print("Before removing NaN values:")
print(f"Total number of samples: {disease_state_df.shape[0]}")
print(f"Total number of genes: {gds_df.shape[0]}")
print()

# Check for NaN values in disease_state_df
print("\nChecking for NaN values in Disease State DataFrame:")
disease_state_nan_counts = disease_state_df.isna().sum()
print(disease_state_nan_counts)
if disease_state_nan_counts.sum() > 0:
    print("NaN values found in Disease State DataFrame. Handling missing values.")
    disease_state_df = disease_state_df.dropna()
print()

# Check for NaN values in gds_df
print("\nChecking for NaN values in GDS DataFrame:")
gds_nan_counts = gds_df.isna().sum()
print(gds_nan_counts)

# Remove genes with more than 20% missing values
threshold = 0.8 * gds_df.shape[1]
gds_df = gds_df.dropna(thresh=threshold, axis=0)
print("\nNaN values after handling in GDS DataFrame:")
gds_nan_counts = gds_df.isna().sum()
print(gds_nan_counts)
print()

# Print total number of samples and genes after removing NaN values
print("\nAfter removing genes with more than 20% missing values:")
print(f"Total number of samples: {disease_state_df.shape[0]}")
print(f"Total number of genes: {gds_df.shape[0]}")
print()

# Print dataframe heads
print("\nDisease State DataFrame Head:")
print(disease_state_df.head())
print()
print("GDS DataFrame Head:")
print(gds_df.head())
print()

# Count number of samples for each class
autism_samples = disease_state_df[disease_state_df['disease state'] == 'autism'].index
control_samples = disease_state_df[disease_state_df['disease state'] == 'control'].index

print(f"Number of autistic samples: {len(autism_samples)}")
print(f"Number of control samples: {len(control_samples)}")
print(f"Number of unique genes (IDENTIFIER): {gds_df['IDENTIFIER'].nunique()}")
print()

# Ensure all samples are in both dataframes
matching_samples = [col for col in gds_df.columns if col in disease_state_df.index]
gene_expression_data = gds_df[matching_samples]

if len(matching_samples) != len(gds_df.columns):
    print(f"Number of samples matched in GDS DataFrame: {len(matching_samples)}")
    print(f"Number of genes after filtering columns: {len(gene_expression_data)}")
else:
    print(f"All samples matched: {len(matching_samples)} samples")
print()

# Normalize gene expression data
gene_expression_data = np.log2(gene_expression_data + 1)

# Remove genes with no variance
filtered_gene_expression_data = gene_expression_data.loc[gene_expression_data.var(axis=1) > 0]
print(f"Number of genes after filtering for variance: {len(filtered_gene_expression_data)}")

# Calculate fold changes
mean_autism = gene_expression_data[autism_samples].mean(axis=1)
mean_control = gene_expression_data[control_samples].mean(axis=1)
fold_changes = mean_autism - mean_control

# Performing t-tests to calculate p-values and adjusting p-values.
p_values = []
filtered_genes = filtered_gene_expression_data.index
for gene in filtered_genes:
    autism_data = filtered_gene_expression_data.loc[gene, autism_samples]
    control_data = filtered_gene_expression_data.loc[gene, control_samples]
    _, p_value = ttest_ind(autism_data, control_data, nan_policy='omit')
    p_values.append(p_value)

p_values = [p if not np.isnan(p) else 1 for p in p_values]
adjusted_p_values = multipletests(p_values, method='fdr_bh')[1]

# Store fold changes, p-values, and adjusted p-values in a dataframe
results_df = pd.DataFrame({
    'ID_REF': filtered_genes,
    'p_value': p_values,
    'adjusted_p_value': adjusted_p_values,
    'fold_change': fold_changes[filtered_genes]
}).merge(gds_df[['IDENTIFIER']], left_index=True, right_index=True)

# Remove genes with adjusted p-value above 0.05
significant_genes_df = results_df[results_df['adjusted_p_value'] < 0.05].copy()

# Add column to specify up or down regulation of genes based on fold change
significant_genes_df.loc[:, 'regulation'] = np.where(significant_genes_df['fold_change'] > 0, 'up', 'down')

# Sort the significant genes by adjusted p-value
significant_genes_df = significant_genes_df.sort_values(by='adjusted_p_value').reset_index(drop=True)

# Output significant genes count and DataFrame
print(f"Number of significantly differentially expressed genes: {len(significant_genes_df)}")
print("Significant Genes DataFrame:")
print(significant_genes_df[['ID_REF', 'IDENTIFIER', 'p_value', 'adjusted_p_value', 'fold_change', 'regulation']])

# Export significant genes DataFrame to CSV
significant_genes_df.to_csv('significant_genes.csv', index=False)

# Set and log random seed
#rfe_seed = np.random.randint(0, 10000)
rfe_seed = 6124
print(f"RFE random seed: {rfe_seed}")
logging.info(f"RFE random seed: {rfe_seed}")

# Prepare data using all significant genes
X = filtered_gene_expression_data.loc[significant_genes_df['ID_REF']].T
y = disease_state_df['disease state'].apply(lambda x: 1 if x == 'autism' else 0)

# Normalize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Handle class imbalance with SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# Define initial models for RFE
initial_models = {
    'GradientBoosting': GradientBoostingClassifier(random_state=42),
    'XGBoost': XGBClassifier(random_state=42)
}

# Function to evaluate the optimal number of features using RFE with multiple models
def evaluate_rfe_features_cv(model_name, model, X_resampled, y_resampled, num_features, n_splits=5, random_state=None):
    rfe = RFE(model, n_features_to_select=num_features, step=1)
    X_rfe = rfe.fit_transform(X_resampled, y_resampled)
    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    cv_scores = cross_val_score(model, X_rfe, y_resampled, cv=cv, scoring='roc_auc')
    return cv_scores.mean(), rfe.support_

# Iterative RFE with Early Stopping
increment = 5
previous_score = 0
tolerance = 0.02
no_improvement_count = 0
max_no_improvement = 4
rfe_scores = []

for num_features in range(increment, len(significant_genes_df) + 1, increment):
    for model_name, model in initial_models.items():
        score, support = evaluate_rfe_features_cv(model_name, model, X_resampled, y_resampled, num_features, random_state=rfe_seed)
        rfe_scores.append((model_name, num_features, score, support))

        if abs(score - previous_score) < tolerance:
            no_improvement_count += 1
        else:
            no_improvement_count = 0

        if no_improvement_count >= max_no_improvement:
            print(f"Early stopping: No significant improvement for {max_no_improvement} increments.")
            break

        previous_score = score

    if no_improvement_count >= max_no_improvement:
        break

# Visualization for number of features vs. ROC-AUC score for each model
for model_name in initial_models.keys():
    model_scores = [score for model, n, score, support in rfe_scores if model == model_name]
    features = [n for model, n, score, support in rfe_scores if model == model_name]
    plt.figure(figsize=(10, 6))
    plt.plot(features, model_scores, marker='o', label=model_name)
    plt.title(f'Number of Features vs. Cross-validated ROC-AUC Score for {model_name}')
    plt.xlabel('Number of Features')
    plt.ylabel('Cross-validated ROC-AUC Score')
    plt.legend()
    plt.show()

# Print the optimal number of features and associated ROC-AUC for each model
for model_name in initial_models.keys():
    model_best_score = max([(model, n, score) for model, n, score, support in rfe_scores if model == model_name], key=lambda x: x[2])
    print(f"Model: {model_best_score[0]}, Optimal number of features: {model_best_score[1]}, ROC-AUC: {model_best_score[2]:.4f}")

# Find the overall optimal model and number of features
best_rfe_score = max(rfe_scores, key=lambda x: x[2])
optimal_rfe_model_name, optimal_num_features, best_rfe_roc_auc, selected_features = best_rfe_score

print(f"Overall Best RFE model: {optimal_rfe_model_name} with {optimal_num_features} features, ROC-AUC: {best_rfe_roc_auc:.4f}")

# Extract the selected features based on the optimal number
selected_indices = np.where(selected_features)[0]

# Ensure selected_indices are within the bounds of significant_genes_df
selected_indices = selected_indices[selected_indices < len(significant_genes_df)]

# Select the optimal genes identified by RFE based on the selected indices
optimal_rfe_genes = significant_genes_df.iloc[selected_indices]

# Create table of optimal RFE features
optimal_rfe_features_df = optimal_rfe_genes[['ID_REF']]
print("Optimal RFE Features:")
print(optimal_rfe_features_df)

# Log the selected RFE features
logging.info(f"Optimal RFE model: {optimal_rfe_model_name}")
logging.info(f"Number of selected features: {optimal_num_features}")
logging.info(f"Selected RFE features:\n{optimal_rfe_features_df}")

# Export the optimal RFE features to CSV
optimal_rfe_features_df.to_csv('optimal_rfe_features.csv', index=True)

# Prepare data using the RFE-selected features
X_rfe = X_resampled[:, selected_indices]
y_rfe = y_resampled

# Suppress Optuna's trial logs
optuna.logging.set_verbosity(optuna.logging.WARNING)

# Define a function for optimizing models with Optuna
def optimize_model(trial, model_name, X, y):
    if model_name == 'GradientBoosting':
        n_estimators = trial.suggest_int('n_estimators', 50, 300)
        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.1, log=True)
        max_depth = trial.suggest_int('max_depth', 3, 10)
        model = GradientBoostingClassifier(
            n_estimators=n_estimators,
            learning_rate=learning_rate,
            max_depth=max_depth,
            n_iter_no_change=10,
            random_state=42
        )
    elif model_name == 'XGBoost':
        n_estimators = trial.suggest_int('n_estimators', 50, 300)
        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.1, log=True)
        max_depth = trial.suggest_int('max_depth', 3, 10)
        eval_metric = trial.suggest_categorical('eval_metric', ['logloss', 'auc', 'rmse'])
        model = XGBClassifier(
            n_estimators=n_estimators,
            learning_rate=learning_rate,
            max_depth=max_depth,
            random_state=42,
            use_label_encoder=False,
            eval_metric=eval_metric
        )
    elif model_name == 'SVM':
        C = trial.suggest_float('C', 0.01, 10, log=True)
        kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])
        model = SVC(C=C, kernel=kernel, probability=True, random_state=42
        )
    elif model_name == 'MLP':
        hidden_layer_sizes = trial.suggest_int('hidden_layer_sizes', 50, 150)
        activation = trial.suggest_categorical('activation', ['relu', 'tanh'])
        solver = trial.suggest_categorical('solver', ['adam', 'lbfgs'])
        alpha = trial.suggest_float('alpha', 1e-4, 1e-2, log=True)
        learning_rate = trial.suggest_categorical('learning_rate', ['adaptive', 'constant', 'invscaling'])
        max_iter = trial.suggest_int('max_iter', 1000, 3000)
        n_iter_no_change = trial.suggest_int('n_iter_no_change', 5, 20)
        model = MLPClassifier(
            hidden_layer_sizes=(hidden_layer_sizes,),
            activation=activation,
            solver=solver,
            alpha=alpha,
            learning_rate=learning_rate,
            max_iter=max_iter,
            early_stopping=True,
            n_iter_no_change=n_iter_no_change,
            random_state=42
        )
    return cross_val_score(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring='roc_auc').mean()

# Optimize each model
models_to_optimize = ['GradientBoosting', 'XGBoost', 'SVM', 'MLP']
optimized_models = {}

# Iterate through each model to perform hyperparameter optimization using Optuna
for model_name in models_to_optimize:
    print(f"Optimizing {model_name} hyperparameters...")
    # Set random seed for Optuna's sampler
    sampler = optuna.samplers.TPESampler(seed=42)
    study = optuna.create_study(direction='maximize', sampler=sampler)
    study.optimize(lambda trial: optimize_model(trial, model_name, X_rfe, y_rfe), n_trials=50)
    optimized_models[model_name] = study.best_trial.params
    print(f"Best {model_name} model parameters: {optimized_models[model_name]}")
    logging.info(f"Best {model_name} model parameters: {optimized_models[model_name]}")
    print(f"Best {model_name} model ROC-AUC: {study.best_value:.4f}")
    logging.info(f"Best {model_name} model ROC-AUC: {study.best_value:.4f}")

# Optimal parameters for final models
final_models = {
    'GradientBoosting': GradientBoostingClassifier(
        **{k: v for k, v in optimized_models['GradientBoosting'].items() if k in ['n_estimators', 'learning_rate', 'max_depth', 'n_iter_no_change']},
        random_state=42
    ),
    'XGBoost': XGBClassifier(
        **{k: v for k, v in optimized_models['XGBoost'].items() if k in ['n_estimators', 'learning_rate', 'max_depth', 'eval_metric']},
        random_state=42,
        use_label_encoder=False
    ),
    'SVM': SVC(
        **{k: v for k, v in optimized_models['SVM'].items() if k in ['C', 'kernel']},
        probability=True,
        random_state=42
    ),
    'MLP': MLPClassifier(
        **{k: v for k, v in optimized_models['MLP'].items() if k in ['hidden_layer_sizes', 'activation', 'solver', 'alpha', 'learning_rate', 'max_iter', 'n_iter_no_change']},
        early_stopping=True,
        random_state=42
    )
}

# Log the final models' parameters
logging.info("Optimal parameters for final models:")
for model_name, model in final_models.items():
    logging.info(f"{model_name}: {model.get_params()}")

# Split the data using the selected features
X_train, X_test, y_train, y_test = train_test_split(X_rfe, y_rfe, test_size=0.3, random_state=42)

# Collect model performance data
model_names = ['GradientBoosting', 'XGBoost', 'SVM', 'MLP']
roc_auc_scores = []
accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []

# Train and evaluate each model
for model_name, model in final_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    acc = accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    classification_rep = classification_report(y_test, y_pred, output_dict=True)

    roc_auc_scores.append(roc_auc)
    accuracy_scores.append(acc)
    precision_scores.append(classification_rep['weighted avg']['precision'])
    recall_scores.append(classification_rep['weighted avg']['recall'])
    f1_scores.append(classification_rep['weighted avg']['f1-score'])

    # Print and log the model's performance
    print(f"Model: {model_name}, F1 Score: {classification_rep['weighted avg']['f1-score']:.4f}, ROC-AUC: {roc_auc:.4f}")
    print(classification_report(y_test, y_pred))
    logging.info(f"Model: {model_name}, F1 Score: {classification_rep['weighted avg']['f1-score']:.4f}, ROC-AUC: {roc_auc:.4f}")
    logging.info(f"Classification Report for {model_name}:\n{classification_report(y_test, y_pred)}")

# Find the best individual model based on ROC-AUC score
individual_best_model = max(final_models.items(), key=lambda model: roc_auc_score(y_test, model[1].predict_proba(X_test)[:, 1]))
individual_best_roc_auc = roc_auc_score(y_test, individual_best_model[1].predict_proba(X_test)[:, 1])

# Print and log the best individual model and its ROC-AUC score
print(f"Best individual model: {individual_best_model[0]}, ROC-AUC: {individual_best_roc_auc:.4f}, F1 Score: {classification_rep['weighted avg']['f1-score']:.4f}")
logging.info(f"Best individual model: {individual_best_model[0]}, ROC-AUC: {individual_best_roc_auc:.4f}, F1 Score: {classification_rep['weighted avg']['f1-score']:.4f}")

# Create a DataFrame for visualization
performance_df = pd.DataFrame({
    'Model': model_names,
    'ROC-AUC': roc_auc_scores,
    'Accuracy': accuracy_scores,
    'Precision': precision_scores,
    'Recall': recall_scores,
    'F1-Score': f1_scores
})

# Melt the DataFrame for easier plotting with seaborn
performance_df_melted = performance_df.melt(id_vars='Model', var_name='Metric', value_name='Score')

# Create the plot
plt.figure(figsize=(14, 8))
sns.barplot(x='Score', y='Model', hue='Metric', data=performance_df_melted)
plt.title('Model Performance Comparison')
plt.xlabel('Score')
plt.ylabel('Model')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()

# Define a function to plot SHAP values for models
def plot_shap_feature_importance(model, model_name, X, feature_names):
    explainer = shap.KernelExplainer(model.predict, X)
    shap_values = explainer.shap_values(X)

    # Log SHAP values
    logging.info(f"SHAP values for {model_name}:")
    shap_importance = np.mean(np.abs(shap_values), axis=0)
    sorted_idx = np.argsort(shap_importance)
    for idx in sorted_idx:
        logging.info(f"Gene: {feature_names[idx]}, SHAP Importance: {shap_importance[idx]:.4f}")

    shap.summary_plot(shap_values, X, feature_names=feature_names, show=False, plot_type='bar')
    plt.title(f"SHAP Feature Importance for {model_name}")
    plt.show()

# Plot SHAP feature importance for all individual models
for model_name, model in final_models.items():
    plot_shap_feature_importance(model, model_name, X_test, optimal_rfe_genes['IDENTIFIER'].values)

# Evaluate all model combinations (2 to 6 models)
model_combinations = []
for i in range(2, 7):
    model_combinations.extend(itertools.combinations(final_models.keys(), i))

print(f"Total number of model combinations to evaluate: {len(model_combinations)}")
logging.info(f"Total number of model combinations to evaluate: {len(model_combinations)}")

# Initialize variables to track the best combination and its score
best_combination = None
best_combination_score = 0

# Store the ROC-AUC scores of all combinations
combination_scores = []

# Evaluate each model combination
for combination in model_combinations:
    combined_pred_proba = np.mean([final_models[model].predict_proba(X_test)[:, 1] for model in combination], axis=0)
    combination_score = roc_auc_score(y_test, combined_pred_proba)
    combination_scores.append((combination, combination_score))

    # Log each combination and its ROC-AUC score
    logging.info(f"Combination: {' + '.join(combination)}, ROC-AUC: {combination_score:.4f}")

    if combination_score > best_combination_score:
        best_combination_score = combination_score
        best_combination = combination

# Sort combinations by ROC-AUC score in descending order
combination_scores.sort(key=lambda x: x[1], reverse=True)

# Print and log the best model combination and its ROC-AUC score
print(f"Best model combination: {best_combination}, ROC-AUC: {best_combination_score:.4f}")
logging.info(f"Best model combination: {best_combination}, ROC-AUC: {best_combination_score:.4f}")

# Generate plot
comb_labels = [' + '.join(comb) for comb, _ in combination_scores]
roc_auc_values = [score for _, score in combination_scores]

fig_width = 12
fig_height = 9

plt.figure(figsize=(fig_width, fig_height))
plt.barh(comb_labels[::-1], roc_auc_values[::-1], color='skyblue')
plt.xlabel('ROC-AUC Score')
plt.ylabel('Model Combinations')
plt.title('ROC-AUC Scores of Model Combinations')
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Determine the best overall model
if best_combination_score > individual_best_roc_auc:
    best_model = best_combination
    best_model_proba = np.mean([final_models[model].predict_proba(X_test)[:, 1] for model in best_combination], axis=0)
    best_model_type = "Ensemble"
    best_model_name = best_combination
    best_model_roc_auc = best_combination_score
else:
    best_model = individual_best_model[0]
    best_model_proba = final_models[best_model].predict_proba(X_test)[:, 1]
    best_model_type = "Individual"
    best_model_name = individual_best_model[0]
    best_model_roc_auc = individual_best_roc_auc

# Print and log the best overall model and its ROC-AUC score
print(f"Best overall model: {best_model_name}, Type: {best_model_type}, ROC-AUC: {best_model_roc_auc:.4f}")
logging.info(f"Best overall model: {best_model_name}, Type: {best_model_type}, ROC-AUC: {best_model_roc_auc:.4f}")

# Perform PCA on the test set
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_test)

# Create a scatter plot of the PCA results colored by predicted probabilities from the best overall model
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=best_model_proba, cmap='coolwarm', alpha=0.7)
plt.title('PCA of Test Set Data Colored by Predicted Probabilities from Best Model')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
cbar = plt.colorbar(scatter)
cbar.set_label('Predicted Probability')
plt.show()

# Display the PCA loadings
loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=optimal_rfe_genes['IDENTIFIER'].values)
print("PCA Loadings for PC1 and PC2:")
print(loadings)
logging.info("PCA Loadings for PC1 and PC2:")
logging.info(loadings.to_string())

# Define a function to plot SHAP values
def plot_shap_values(model, X, model_name, feature_names):
    plt.figure(figsize=(10, 6))

    # Check if the model is tree-based
    if model_name in ['GradientBoosting', 'XGBoost']:
        # Use TreeExplainer for tree-based models
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X)

        # Log SHAP values
        logging.info(f"SHAP values for {model_name}:")
        shap_importance = np.mean(np.abs(shap_values), axis=0)
        sorted_idx = np.argsort(shap_importance)
        for idx in sorted_idx:
            logging.info(f"Gene: {feature_names[idx]}, SHAP Importance: {shap_importance[idx]:.4f}")

        # Plot SHAP summary plot for tree-based models
        shap.summary_plot(shap_values, X, feature_names=feature_names, show=False, plot_type='dot')
        plt.title(f'SHAP Values for {model_name}', fontsize=15)
        plt.xlabel('SHAP value (impact on model output)', fontsize=12)

    # Check if the model is non-tree-based
    elif model_name in ['SVM', 'MLP']:
        # Use KernelExplainer for non-tree-based models
        explainer = shap.KernelExplainer(model.predict, X)
        shap_values = explainer.shap_values(X)

        # Log SHAP values
        logging.info(f"SHAP values for {model_name}:")
        shap_importance = np.mean(np.abs(shap_values), axis=0)
        sorted_idx = np.argsort(shap_importance)
        for idx in sorted_idx:
            logging.info(f"Gene: {feature_names[idx]}, SHAP Importance: {shap_importance[idx]:.4f}")

        # Plot SHAP summary plot for non-tree-based models
        shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)
        plt.title(f'SHAP Values for {model_name}', fontsize=15)
        plt.xlabel('SHAP value (impact on model output)', fontsize=12)

    # Display the plot
    plt.show()

# Check if the best model is an ensemble
if isinstance(best_model, tuple):
    # Compute and plot the combined SHAP values for the ensemble
    combined_shap_values = []
    for model in best_model:
        if model in ['GradientBoosting', 'XGBoost']:
            explainer = shap.TreeExplainer(final_models[model])
            shap_vals = explainer.shap_values(X_test)
            combined_shap_values.append(shap_vals)

    if combined_shap_values:
        combined_shap_values = np.mean(np.array(combined_shap_values), axis=0)
        shap.summary_plot(combined_shap_values, X_test, feature_names=optimal_rfe_genes['IDENTIFIER'].values)
        # Log SHAP values for the ensemble
        logging.info(f"SHAP values for Ensemble model: {best_model_name}")
else:
    # Plot SHAP values for the best individual model
    plot_shap_values(final_models[best_model], X_test, best_model, optimal_rfe_genes['IDENTIFIER'].values)
    # Log SHAP values for the individual model
    logging.info(f"SHAP values for Individual model: {best_model_name}")

# Calculate the average ROC-AUC for all model combinations
average_combination_roc_auc = np.mean([score for _, score in combination_scores])

# Calculate the average ROC-AUC for all individual models
average_individual_roc_auc = performance_df['ROC-AUC'].mean()

# Log the average ROC-AUC values
logging.info(f"Average ROC-AUC for all model combinations: {average_combination_roc_auc:.4f}")
logging.info(f"Average ROC-AUC for all individual models: {average_individual_roc_auc:.4f}")

# Print the average ROC-AUC values
print(f"Average ROC-AUC for all model combinations: {average_combination_roc_auc:.4f}")
print(f"Average ROC-AUC for all individual models: {average_individual_roc_auc:.4f}")

# Visualize average ROC-AUC for combinations and individual models
avg_roc_auc_data = {
    'Type': ['Combinations', 'Individual Models'],
    'Average ROC-AUC': [average_combination_roc_auc, average_individual_roc_auc]
}

avg_roc_auc_df = pd.DataFrame(avg_roc_auc_data)

# Create the bar plot with thinner bars and adjusted plot space
plt.figure(figsize=(6, 4))
sns.barplot(x='Type', y='Average ROC-AUC', data=avg_roc_auc_df, hue='Type', dodge=False, palette='viridis', width=0.5)
plt.title('Average ROC-AUC Comparison')
plt.ylabel('Average ROC-AUC')
plt.xlabel('Model Type')
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.legend([], [], frameon=False)
plt.tight_layout()
plt.show()

# Function to convert log file to CSV
def log_to_csv(log_file, csv_file):
    with open(log_file, 'r') as file:
        lines = file.readlines()

    # Split lines into columns
    data = [line.strip().split(' - ') for line in lines if ' - ' in line]
    df = pd.DataFrame(data, columns=['Timestamp', 'Message'])

    # Save to CSV
    df.to_csv(csv_file, index=False)

# Convert the log file to CSV
csv_file_path = os.path.join(log_dir, 'run_log_01.csv')
log_to_csv(log_file_path, csv_file_path)

# Verify CSV creation
if os.path.exists(csv_file_path):
    print(f"Log file converted to CSV and saved at {csv_file_path}")
else:
    print("Failed to convert log file to CSV.")

end_time = time.time()
elapsed_time = end_time - start_time
elapsed_time_minutes = elapsed_time / 60
print(f"Elapsed time: {elapsed_time_minutes:.2f} minutes")